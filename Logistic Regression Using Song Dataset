setwd("E:/R & Python Documents/R Machine Learning/Models Built/Logistic Regression")

df=read.csv("songs.csv")

# How many observations (songs) are from the year 2010?
dim(df)#1.1
colnames(df)
View(df[ which(df$year=='2010'),])
length(df[ which(df$year=='2010'),])

#How many songs does the dataset include for which the artist name is "Michael Jackson"?
length(which(df$artistname=="Michael Jackson"))#1.2
#or
sum(df$artistname=="Michael Jackson")
#or
View(df[ which(df$artistname=='Michael Jackson'),])

# Which of these songs by Michael Jackson made it to the Top 10

df$songtitle[(df$artistname=="Michael Jackson" & df$Top10==1)] #1.3
#You Rock My World, You Are Not Alone, Black or White, Remember the Time & In The Closet   
#or 
View(df[ which(df$artistname=='Michael Jackson' & df$Top10==1),])

# What are the timesignature that occur in our dataset
a =df$year[order(df$year)]
apply(df,2, function(x){sort(order(x))})
summary(df)
View(df)
head(a)
table(df$timesignature)#1.4
#or
unique(df$timesignature)

#Out of all of the songs in our dataset, the song with the highest tempo 
#is one of the following songs. Which one is it?
df$songtitle[which.max(df$tempo)]#1.5
#Wanna Be Startin' Somethin'

#************************Problem 2************************
# Splitting the data into train and test based on the Year
songsTrain=subset(df,df$year<=2009)
songsTest=subset(df,df$year==2010)
dim(songsTrain)#2.1

# Removing Unwanted Columns 
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
songsTrain=songsTrain[,!colnames(songsTrain) %in% (nonvars)]
songsTest=songsTest[,!colnames(songsTest) %in% (nonvars)]

Model1=glm(Top10~ .,data=songsTrain,family="binomial")
summary(Model1)#2.2

#2.3
#timesignature_confidence, tempo_confidence & key_confidence all have positive coefficients
#that means higher their values the more you go towards P==1. This come sfrom the S curve..
#watch lecture video for this..
#The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10

#2.4
#Explanation
#Since the coefficient values for timesignature_confidence, tempo_confidence, and key_confidence
#are all positive, lower values leads to a lower predicted probability of Top10==1 i.e.
#of a song being a hit. So mainstream (most) listeners tend to prefer less complex songs.

#2.5
#The coefficient estimate for loudness is positive, meaning that mainstream listeners prefer louder
#songs, which are those with heavier instrumentation.

#However, the coefficient estimate for energy is negative, meaning that mainstream listeners prefer songs that are less energetic, which are those with light instrumentation.
#These coefficients lead us to different conclusions!


#2.6
attach(songsTrain) #y to use this..google :P
cor(loudness,energy)
#*******************************Problem- 3********************
Model2=glm(Top10~ .-loudness,data=songsTrain,family="binomial")
Model3=glm(Top10~ .-energy,data=songsTrain,family="binomial")

#3.2
SongsLog2 = glm(Top10 ~ . - loudness, data=songsTrain, family=binomial)
summary(SongsLog2)

#Model 2 suggests that songs with high energy levels tend to be more popular.
#This contradicts our observation in Model 1.

#The coefficient estimate for energy is positive in Model 2, suggesting that songs with higher energy levels tend to be more popular.
#However, note that the variable energy is not significant in this model.


#3.3
Model3=glm(Top10~ .-energy,data=songsTrain,family="binomial")
summary(Model3)
#Looking at the output of summary(SongsLog3), we can see that loudness has a
#positive coefficient estimate, meaning that our model predicts that songs with heavier
#instrumentation tend to be more popular. This is the same conclusion we got from Model 2.

#4.1
pred=predict(Model3,newdata=songsTest,type="response")
pred=ifelse(pred>=0.45,1,0)
table(songsTest$Top10,pred)
accuracy=((309+19)/nrow(songsTest))
#87.93

#4.2
#baseline accuracy on train data is prop.table(table(songsTrain$Top10)).so 85.27%
prop.table(table(songsTest$Top10))
#84.18%


#4.3
table(songsTest$Top10,pred)
#true positive is 19
#False positive is 5

#4.4
Sensitivity=19/(19+40)
#32.20%

Specifictiy=309/(309+5)
#98.4%


#4.5
#Model 3 favors specificity over sensitivity.
#Model 3 provides conservative predictions, and predicts that a song
#will make it to the Top 10 very rarely. So while it detects less than 
#half of the Top 10 songs, we can be very confident in the songs that it does predict to be Top 10 hits.



#While Model 3 only captures less than half of the Top 10 songs,
#it still can offer a competitive edge, since it is very conservative in its predictions.
#*******************************************WELL doNE GUYS*****************************************
